{"cells":[{"cell_type":"markdown","id":"eb2edd30-3ef7-421f-a4a3-d406610b7b3d","metadata":{"id":"eb2edd30-3ef7-421f-a4a3-d406610b7b3d"},"source":["# NLP. Lab 1. Tokenization.\n"]},{"cell_type":"markdown","id":"f0eae649-8e0a-45e0-b353-3f4ea6a6d422","metadata":{"id":"f0eae649-8e0a-45e0-b353-3f4ea6a6d422"},"source":["## What is tokenization?\n"]},{"cell_type":"markdown","id":"a354d7d4-3728-4636-b876-1ae25ca1e795","metadata":{"id":"a354d7d4-3728-4636-b876-1ae25ca1e795"},"source":["Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization. We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\n","\n","![NLP_Tokenization](https://raw.githubusercontent.com/satishgunjal/images/master/NLP_Tokenization.png)\n"]},{"cell_type":"markdown","id":"d72c749d-fb39-4747-8c3e-43f88b7013b3","metadata":{"id":"d72c749d-fb39-4747-8c3e-43f88b7013b3"},"source":["### Purpose\n"]},{"cell_type":"markdown","id":"71a7ec80-d1a4-4823-9c26-849e5289a9f8","metadata":{"id":"71a7ec80-d1a4-4823-9c26-849e5289a9f8"},"source":["Every sentence gets its meaning by the words present in it. So by analyzing the words present in the text we can easily interpret the meaning of the text. Once we have a list of words we can also use statistical tools and methods to get more insights into the text. For example, we can use word count and word frequency to find out important of word in that sentence or document.\n"]},{"cell_type":"markdown","id":"3bd9b8fc-4976-41f4-97d9-2464cd50b122","metadata":{"id":"3bd9b8fc-4976-41f4-97d9-2464cd50b122"},"source":["## Tokenization in Python\n"]},{"cell_type":"code","execution_count":1,"id":"260412eb-3fc2-465a-aa40-8d9f9221fdc8","metadata":{"id":"260412eb-3fc2-465a-aa40-8d9f9221fdc8","executionInfo":{"status":"ok","timestamp":1705997527368,"user_tz":-180,"elapsed":5,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["text = \"Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization.  We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\""]},{"cell_type":"markdown","id":"5ee832e5-6f35-4f36-9adb-4016f4290bb4","metadata":{"id":"5ee832e5-6f35-4f36-9adb-4016f4290bb4"},"source":["### Built-in methods\n"]},{"cell_type":"markdown","id":"fae3eb88-9617-488c-8df8-3e185b7acb53","metadata":{"id":"fae3eb88-9617-488c-8df8-3e185b7acb53"},"source":["We can use **split()** method to split a string into a list where each word is a list item.\n"]},{"cell_type":"markdown","id":"7113b99f-7fe1-4431-872e-41cb42ea2d03","metadata":{"id":"7113b99f-7fe1-4431-872e-41cb42ea2d03"},"source":["#### Word tokenization\n"]},{"cell_type":"code","execution_count":2,"id":"cf492d98-e69b-4164-ab15-5da3eb3f620b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf492d98-e69b-4164-ab15-5da3eb3f620b","executionInfo":{"status":"ok","timestamp":1705997527368,"user_tz":-180,"elapsed":4,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"d0c31315-2855-435d-9f0c-90c3ac7a7c5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'one', 'of', 'the']\n"]}],"source":["tokens = text.split()\n","print(tokens[:5])"]},{"cell_type":"markdown","id":"ba184a73-0600-4eb9-9653-1b2f5ad88a6c","metadata":{"id":"ba184a73-0600-4eb9-9653-1b2f5ad88a6c"},"source":["#### Sentence tokenization\n"]},{"cell_type":"code","execution_count":3,"id":"94809ca6-daff-4c98-ae0d-a739c5b4096e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94809ca6-daff-4c98-ae0d-a739c5b4096e","executionInfo":{"status":"ok","timestamp":1705997534343,"user_tz":-180,"elapsed":3,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"deac0def-cbbc-4503-a60d-cd5cc00fc740"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is one of the first step in any NLP pipeline', ' Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens', \" If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'\"]\n"]}],"source":["tokens = text.split(\".\")\n","print(tokens[:3])"]},{"cell_type":"markdown","id":"a46b238e-1d54-49e5-80f1-e995cd5dbe88","metadata":{"id":"a46b238e-1d54-49e5-80f1-e995cd5dbe88"},"source":["### RegEx tokenization\n","\n","- Using RegEx we can match character combinations in string and perform word/sentence tokenization.\n","- You can check your regular expressions at [regex101](https://regex101.com/)\n"]},{"cell_type":"markdown","id":"2c4e4c3f-bdd2-487d-9103-4f53bc65d78e","metadata":{"id":"2c4e4c3f-bdd2-487d-9103-4f53bc65d78e"},"source":["#### Word tokenization\n"]},{"cell_type":"code","execution_count":4,"id":"3fe23141-8fb7-4847-837a-b8c9919b3329","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fe23141-8fb7-4847-837a-b8c9919b3329","executionInfo":{"status":"ok","timestamp":1705997541112,"user_tz":-180,"elapsed":2,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"11a3c22e-7b59-468b-82bb-7e6de56a95b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'one', 'of', 'the']\n"]}],"source":["import re\n","\n","tokens = re.findall(\"[\\w]+\", text)\n","print(tokens[:5])"]},{"cell_type":"markdown","id":"75296723-818a-4f79-bf19-2f6667d6bbc5","metadata":{"id":"75296723-818a-4f79-bf19-2f6667d6bbc5"},"source":["### NLTK library\n"]},{"cell_type":"markdown","id":"5acd0a61-4237-4e76-bdb9-64300e258004","metadata":{"id":"5acd0a61-4237-4e76-bdb9-64300e258004"},"source":["#### Word tokenization\n"]},{"cell_type":"code","execution_count":5,"id":"5697f541-8d47-405b-93d1-088b3fd2c4a7","metadata":{"id":"5697f541-8d47-405b-93d1-088b3fd2c4a7","executionInfo":{"status":"ok","timestamp":1705997554550,"user_tz":-180,"elapsed":1,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":7,"id":"ac8af61d-395c-489d-ad2f-dd4d3ba1523b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac8af61d-395c-489d-ad2f-dd4d3ba1523b","executionInfo":{"status":"ok","timestamp":1705997591557,"user_tz":-180,"elapsed":1453,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"dbbbf6fb-8cd5-4c7f-936f-dcfa247acd47"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'one', 'of', 'the']\n"]}],"source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","tokens = word_tokenize(text)\n","print(tokens[:5])"]},{"cell_type":"markdown","id":"418269d2-db38-42f6-8c5f-ae19c16530a5","metadata":{"id":"418269d2-db38-42f6-8c5f-ae19c16530a5"},"source":["#### Sentence tokenization\n"]},{"cell_type":"code","execution_count":8,"id":"afd2e844-f17f-44c3-8547-aea3056b2881","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afd2e844-f17f-44c3-8547-aea3056b2881","executionInfo":{"status":"ok","timestamp":1705997599159,"user_tz":-180,"elapsed":343,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"9262bc46-6218-496f-e84b-881b1bf3cd03"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is one of the first step in any NLP pipeline.', 'Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.', \"If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.\"]\n"]}],"source":["from nltk.tokenize import sent_tokenize\n","\n","tokens = sent_tokenize(text)\n","print(tokens[:3])"]},{"cell_type":"markdown","id":"912c59f7-eea3-4032-8db8-7638122d0ff1","metadata":{"id":"912c59f7-eea3-4032-8db8-7638122d0ff1"},"source":["### Spacy library\n"]},{"cell_type":"markdown","id":"c7ff4df4-c446-4bf7-8aa7-0709e7e9fe4f","metadata":{"id":"c7ff4df4-c446-4bf7-8aa7-0709e7e9fe4f"},"source":["#### Word tokenization\n"]},{"cell_type":"code","execution_count":null,"id":"154868b4-915c-45c0-b282-e716c4c0cfe4","metadata":{"id":"154868b4-915c-45c0-b282-e716c4c0cfe4"},"outputs":[],"source":["!pip install spacy\n","!python -m spacy download en"]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"id":"kIJayVv3bKmB"},"id":"kIJayVv3bKmB","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"id":"4b0acd17-2b5c-49de-819a-a75384d14ded","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b0acd17-2b5c-49de-819a-a75384d14ded","executionInfo":{"status":"ok","timestamp":1705997666493,"user_tz":-180,"elapsed":9618,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"d3649b9d-7afd-4f5d-9714-d097e7546fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'one', 'of', 'the']\n"]}],"source":["from spacy.lang.en import English\n","\n","english_tokenizer = English()\n","\n","doc = english_tokenizer(text)\n","tokens = [token.text for token in doc]\n","print(tokens[:5])"]},{"cell_type":"markdown","id":"e8b026f2-895a-4db0-bbfc-7ab8cbf5cb2f","metadata":{"id":"e8b026f2-895a-4db0-bbfc-7ab8cbf5cb2f"},"source":["#### Sentence tokenization\n"]},{"cell_type":"code","execution_count":11,"id":"80baf370-9bd4-4243-84da-8ccbf361ee57","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80baf370-9bd4-4243-84da-8ccbf361ee57","executionInfo":{"status":"ok","timestamp":1705997717023,"user_tz":-180,"elapsed":307,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"5e8d6198-2bd8-4fa7-9394-0a45d4c59658"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Tokenization is one of the first step in any NLP pipeline., Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens., If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.]\n"]}],"source":["english_tokenizer = English()\n","english_tokenizer.add_pipe(\"sentencizer\")\n","\n","\n","doc = english_tokenizer(text)\n","tokens = [token.sent for token in doc.sents]\n","print(tokens[:3])"]},{"cell_type":"markdown","id":"0828e650-0fc3-4365-95d2-07a76e5dbb34","metadata":{"id":"0828e650-0fc3-4365-95d2-07a76e5dbb34"},"source":["## Task\n"]},{"cell_type":"markdown","id":"afa023e4-485a-4335-834e-33aa46a1ec80","metadata":{"id":"afa023e4-485a-4335-834e-33aa46a1ec80"},"source":["Your goal is to solve tokenization task and count number of numeric tokens.\n","\n","You should submit your solution to [competition](https://www.kaggle.com/t/50b3669520ce4a0e892900406bbc1f2f).\n"]},{"cell_type":"markdown","id":"d13a11c1-1478-477f-b622-907a262d3c8b","metadata":{"id":"d13a11c1-1478-477f-b622-907a262d3c8b"},"source":["### Grade distribution\n","\n","- your solution is ranked above or the same as the benchmark solution in the leaderboard - 1 point\n","- your solution is lower than the benchmark solution - 0.5 points\n","- no submission / late submission / no appearance on leaderboard - 0 points\n"]},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"8JxgNZUfbqoo","executionInfo":{"status":"ok","timestamp":1705998467121,"user_tz":-180,"elapsed":274,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"id":"8JxgNZUfbqoo","execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":26,"id":"86e09c29","metadata":{"id":"86e09c29","executionInfo":{"status":"ok","timestamp":1705999490919,"user_tz":-180,"elapsed":983,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["text = \"\"\"Barack Obama was the 44th president of the US and he followed George W. Bush and was followed by Donald Trump in 2017.\n","As a young man, George H.W. Bush served in World War II as a fighter pilot. In 1944, he was shot down and had to parachute to safety.\n","Before he was president, George W. Bush was a cheerleader, a fraternity brother, an oilman, an owner of a professional baseball team, and a governor. After leaving office in 2009, Bush learned to paint.\n","Here's something else you probably didn't know about John Adams: He died on the Fourth of July. And he wasn't the only commander in chief to do so. In fact, three of the nation's five founding fathers—Adams, Thomas Jefferson, and James Monroe—died on Independence Day. Adams and Jefferson even passed on the same exact day: July 4, 1826, which happened to be the 50th anniversary of the adoption of the Declaration of Independence.\n","At 6 feet 4 inches tall, Abraham Lincoln and Lyndon B. Johnson were America's tallest presidents. But what about America's shortest president? That distinction goes to founding father James Madison (1809-1817), who, at 5 feet 4 inches tall, was a full foot shorter than his tallest peers.\n","That changed, however, in October 1860, when Lincoln received a letter from an 11-year-old girl named Grace Bedell. 'If you will let your whiskers grow I will try and get [my brothers] to vote for you,' Bedell wrote to Lincoln. 'You would look a great deal better for your face is so thin. All the ladies like whiskers and they would tease their husbands to vote for you and then you would be president'.\n","Richard Nixon was hardly the first president who liked to unwind by rolling a few strikes. Harry S. Truman also enjoyed bowling, and opened the first White House bowling alley in 1947.\n","If you had to bet on which U.S. president was the biggest movie fan, you'd probably put your money on America's actor-turned-president, Ronald Reagan (1981-1989). And that would be a great guess. Reagan reportedly watched 363 movies during his two terms in office.\n","Thomas Jefferson offered to sell his personal library when the Library of Congress was burned by the British during the War of 1812. He sold them 6487 books from his own collection, the largest in America at the time.\n","Born in New York in 1782, Martin Van Buren was the first president to have been born after the American Revolution, technically making him the first American-born president.\n","Benjamin Harrison had a tight-knit family and loved to amuse and dote on his grandchildren. He put up the first recorded White House Christmas tree in 1889, and was known to put on the Santa suit for entertainment.\n","A 16-year-old Bill Clinton managed to shake hands with President John F. Kennedy at a Boys Nation event in 1963. This would take place just four months before Kennedy's assassination.\n","In 1993—two years before he became the governor of Texas—George W. Bush ran the Houston marathon, finishing with a time of 3:44:52. He is the only president to have ever run a marathon.\"\"\"\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","array_text = text.split('\\n')\n","# print(array_text)\n","\n","counts = []\n","\n","for string in array_text:\n","    count = 0\n","    doc = nlp(string)\n","    for token in doc:\n","        if any(char.isdigit() for char in token.text):\n","          # print(token)\n","          count += 1\n","    counts.append(count)\n"]},{"cell_type":"code","execution_count":27,"id":"85f222c2-b0d0-400f-b0d5-4bb92ef8e28f","metadata":{"id":"85f222c2-b0d0-400f-b0d5-4bb92ef8e28f","executionInfo":{"status":"ok","timestamp":1705999491612,"user_tz":-180,"elapsed":2,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["with open(\"submission.csv\", \"w\") as f:\n","    f.write(\"id,count\\n\")\n","    for id, count in enumerate(counts):\n","        f.write(f\"{id},{count}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}